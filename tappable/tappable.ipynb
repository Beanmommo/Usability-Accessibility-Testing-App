{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lRIs1xKv9Cqb",
      "metadata": {
        "id": "lRIs1xKv9Cqb"
      },
      "source": [
        "https://github.com/google-research/google-research/tree/master/taperception\n",
        "\n",
        "https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517497#BibPLXBIB0037\n",
        "\n",
        "Adding notebook to github for version control. \n",
        "Download json_dir and image_dir from google drive to local, or run in Google colab and mount drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hTr0SCVXoV7L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTr0SCVXoV7L",
        "outputId": "4e57ba9c-df1e-4e84-f9c4-36a5fc0ff50d"
      },
      "outputs": [],
      "source": [
        "pip install torchvision torchmetrics numpy pandas scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ec2d4847",
      "metadata": {
        "id": "ec2d4847"
      },
      "outputs": [],
      "source": [
        "#Imports \n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "import torchvision.models as models\n",
        "from torchmetrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "BA1l0kE4xf7L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA1l0kE4xf7L",
        "outputId": "0294552a-f853-455e-f661-4398691228ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x126e268f0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fcf032a1",
      "metadata": {
        "id": "fcf032a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14781"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/google-research-datasets/taperception/main/rico_tap_annotations_idsonly.csv')\n",
        "dataset_test = dataset[dataset['split']=='test']\n",
        "dataset_train = dataset[dataset['split']=='train']\n",
        "\n",
        "json_dir = \"/Users/em.ily/Downloads/json/\"\n",
        "img_dir = \"/Users/em.ily/Downloads/combined/\"\n",
        "\n",
        "len(dataset_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ef5bf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#HYPERPARAMETER TURNING & DATASET SIZE\n",
        "\n",
        "# #full dataset\n",
        "# batch_sze = 1024\n",
        "# epoch_sze = 1500\n",
        "\n",
        "#smaller dataset (change values as you please)\n",
        "batch_sze = 1\n",
        "epoch_sze = 15\n",
        "dataset_test = dataset_test[0:int(np.ceil(len(dataset_test)/10))].reset_index(drop=True)\n",
        "dataset_train = dataset_train[0:int(np.ceil(len(dataset_train))/10)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdb0c42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ebdb0c42",
        "outputId": "fcd7a926-eb22-4de8-d068-be2087df78ef"
      },
      "outputs": [],
      "source": [
        "dataset_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a611fac",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39c2c58e",
      "metadata": {
        "id": "39c2c58e"
      },
      "outputs": [],
      "source": [
        "class image:\n",
        "    \"\"\"\n",
        "    stores the image bounds and related json file to each image. \n",
        "    stores all existing image objects in array. \n",
        "    \"\"\"\n",
        "\n",
        "    _all = []\n",
        "    \n",
        "    def __init__(self, image_id, json_file):\n",
        "        self.image_id = image_id\n",
        "        self.json_file = json_file\n",
        "        self.image_bounds = self.json_file['activity']['root']['rel-bounds']\n",
        "        self.image_bounds_height = self.image_bounds[3]\n",
        "        self.image_bounds_width = self.image_bounds[2]\n",
        "\n",
        "        image._all.append(self)\n",
        "\n",
        "    @classmethod\n",
        "    def all(cls):\n",
        "        return cls._all\n",
        "    \n",
        "    def get_image_bounds_height(self):\n",
        "        return self.image_bounds_height\n",
        "\n",
        "    def get_image_bounds_width(self):\n",
        "        return self.image_bounds_width\n",
        "\n",
        "    def get_json_file(self):\n",
        "        return self.json_file\n",
        "\n",
        "    def list_objects(self):\n",
        "        \"\"\"\n",
        "        returns list of all objects owned by image\n",
        "        \"\"\"\n",
        "        image_objects = [img_obj for img_obj in img_obj.all() if img_obj.owner == self]\n",
        "        return image_objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f169236c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class img_obj:\n",
        "    \"\"\"\n",
        "    finds the bounds of an object within an image using the image json file. \n",
        "    stores object bounds. \n",
        "    stores all existing objects in array.\n",
        "    \"\"\"\n",
        "\n",
        "    _all = []\n",
        "\n",
        "    def __init__(self, object_id, image):\n",
        "        #object\n",
        "        self.object_bounds = []\n",
        "        self.object_id = object_id\n",
        "        self.image = image\n",
        "\n",
        "        img_obj._all.append(self)\n",
        "        self.find_object_bounds(self.image.get_json_file()) #calls method to find object bounds\n",
        "\n",
        "    @classmethod\n",
        "    def all(cls):\n",
        "        return cls._all\n",
        "\n",
        "    def find_object_bounds(self, dict_file):\n",
        "        \"\"\"\n",
        "        recurring method which loops through dictionary file to find the object id and returns bounds\n",
        "        TODO: fix this method. there has to be a better way. \n",
        "        \"\"\"\n",
        "        for key in dict_file:\n",
        "            if key == 'pointer' and dict_file['pointer'] == self.object_id:\n",
        "                self.set_object_bounds(dict_file['bounds'])\n",
        "            if isinstance(dict_file[key], dict):\n",
        "                self.find_object_bounds(dict_file[key])\n",
        "            elif isinstance(dict_file[key], list):\n",
        "                self.find_object_bounds_lst(dict_file[key])\n",
        "\n",
        "    def find_object_bounds_lst(self, lst):\n",
        "        \"\"\"\n",
        "        recurring method which loops through list to find the object id and return bounds\n",
        "        \"\"\"\n",
        "        for item in lst:\n",
        "            if isinstance(item, dict):\n",
        "                self.find_object_bounds(item)\n",
        "            elif isinstance(item, list):\n",
        "                self.find_object_bounds_lst(item)\n",
        "\n",
        "    def get_object_bounds(self):\n",
        "        return self.object_bounds\n",
        "\n",
        "    def set_object_bounds(self, new_bounds):\n",
        "        self.object_bounds = new_bounds\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "691e0e84",
      "metadata": {
        "id": "691e0e84"
      },
      "outputs": [],
      "source": [
        "# Image tranformation\n",
        "class applyMask(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Matrix multiplication of the RGB image and a binary mask of the object\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \n",
        "        image = sample['image']\n",
        "        \n",
        "        binary_mask = np.zeros(shape=(image.shape[0], image.shape[1]))\n",
        "        x_min = sample['x_min']\n",
        "        x_max = sample['x_max']\n",
        "        y_min = sample['y_min']\n",
        "        y_max = sample['y_max']\n",
        "        \n",
        "        for y in range(y_min, y_max):\n",
        "            for x in range(x_min, x_max):\n",
        "                    binary_mask[y,x] = 1 #sets binary mask value to 1 if within tappable bounds\n",
        "\n",
        "        concat = np.dstack((image, binary_mask)) #matrix multiplication of image and binary mask\n",
        "    \n",
        "        return {'image': concat, 'label': sample['label']}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'label': label}\n",
        "\n",
        "dataTransform = transforms.Compose([applyMask(), ToTensor()])\n",
        "\n",
        "class Tappable(Dataset):\n",
        "    \"\"\"\n",
        "    Creates dataset from the csv of labelled image and object ids\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,root_dir, dataset):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = dataTransform\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # check if tensor is saved\n",
        "        tensor_path = 'D:/tappability_tensor_data/' + str(idx) + '.pt'\n",
        "        if path.exists(tensor_path):\n",
        "            return torch.load(tensor_path)\n",
        "        \n",
        "        image_id = self.dataset.iloc[idx, 0]\n",
        "        object_id = self.dataset.iloc[idx, 1]\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, str(image_id) + \".jpg\")\n",
        "        image = io.imread(img_name)\n",
        "        image = transform.resize(image, (960, 540))\n",
        "        \n",
        "        label = self.dataset.iloc[idx, 2]\n",
        "\n",
        "        x_min = self.dataset.iloc[idx, 5]\n",
        "        y_min = self.dataset.iloc[idx, 6]\n",
        "        x_max = self.dataset.iloc[idx, 7]\n",
        "        y_max = self.dataset.iloc[idx, 8]\n",
        "        \n",
        "        sample = { 'image': image, 'image_id': image_id, 'object_id': object_id, 'label': int(label), \n",
        "                    'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max }\n",
        "        \n",
        "        if self.transform:\n",
        "            sample_out = self.transform(sample)\n",
        "\n",
        "        # Comment this out if you don't want 200gb of tensors saved on your computer\n",
        "        torch.save(sample_out, tensor_path)\n",
        "        return sample_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a1e7f4a",
      "metadata": {
        "id": "1a1e7f4a"
      },
      "outputs": [],
      "source": [
        "class Tappable(Dataset):\n",
        "    \"\"\"\n",
        "    Creates dataset from the csv of labelled image and object ids\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,root_dir, dataset, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image_id = self.dataset.iloc[idx, 0]\n",
        "        object_id = self.dataset.iloc[idx, 1]\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                str(image_id) + \".jpg\", )\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        image = transform.resize(image, (540, 960))\n",
        "        \n",
        "        label = self.dataset.iloc[idx, 2]\n",
        "        \n",
        "        sample = {'image': image, 'image_id': image_id, 'object_id': object_id, 'label': int(label) }\n",
        "        \n",
        "        if self.transform:\n",
        "            sample_out = self.transform(sample)\n",
        "\n",
        "        return sample_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "gbq-rDlFYQ69",
      "metadata": {
        "id": "gbq-rDlFYQ69"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#resnet18 cnn model \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.expansion = 1\n",
        "        # self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        # self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.expansion = 1\n",
        "        layers = [2, 2, 2, 2] #resnet 18\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNetLayers\n",
        "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        layers = []\n",
        "\n",
        "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
        "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
        "        self.in_channels = intermediate_channels * self.expansion\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464161d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Tappable(dataset= dataset_train,\n",
        "                         root_dir= img_dir,\n",
        "                         transform=transforms.Compose([\n",
        "                         applyMask(),\n",
        "                         ToTensor()\n",
        "                         ]))\n",
        "\n",
        "test_dataset = Tappable(dataset= dataset_test,\n",
        "                        root_dir= img_dir,\n",
        "                        transform=transforms.Compose([\n",
        "                        applyMask(),\n",
        "                        ToTensor()\n",
        "                        ]))\n",
        "#batch size should be 1024\n",
        "dataloader_train = DataLoader(train_dataset,batch_size=batch_sze, shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)\n",
        "dataloader_test = DataLoader(test_dataset,batch_size=batch_sze,shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "616441da",
      "metadata": {
        "id": "616441da"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2824174408.py, line 9)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Input \u001b[0;32mIn [21]\u001b[0;36m\u001b[0m\n\u001b[0;31m    lambda_lr = lambda epoch: 0.1 ** epoch if epoch%==0 else 1\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(18, Block, 4, 2)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#lr - reduces epoch by factor 10 at specific epoch times\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum = 0.9)\n",
        "lambda_lr = lambda epoch: 0.1 ** epoch if epoch%==0 else 1\n",
        "lr_scheduler = optim.lr_scheduler.LambdaLR(\n",
        "   optimizer=optimizer,\n",
        "   lr_lambda = lambda_lr\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89da90f1",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "89da90f1",
        "outputId": "a4293eeb-724d-46b3-f1ef-fad32583ece1"
      },
      "outputs": [],
      "source": [
        "n_epochs = epoch_sze\n",
        "test_loss = []\n",
        "train_loss = []\n",
        "total_step_train = len(dataloader_train)\n",
        "total_step_test = len(dataloader_test)\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f'Epoch {epoch}\\n')\n",
        "    for batch_idx, item in enumerate(dataloader_train):\n",
        "        inputs, labels = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx) % 20 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}'\n",
        "                   .format(epoch, n_epochs, batch_idx, total_step_train, loss.item()))\n",
        "    train_loss.append(running_loss/total_step_train)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    batch_loss = 0\n",
        "    for batch_idx, item in enumerate(dataloader_test):\n",
        "      data_t, target_t = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "      outputs_t = model(data_t)\n",
        "      loss_t = criterion(outputs_t, target_t)\n",
        "      batch_loss += loss_t.item()\n",
        "      if (batch_idx) % 20 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Test Loss: {:.4f}'\n",
        "                   .format(epoch, n_epochs, batch_idx, total_step_test, loss_t.item()))\n",
        "\n",
        "    test_loss.append(batch_loss/total_step_test)\n",
        "\n",
        "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, test-loss: {np.mean(test_loss):.4f}')\n",
        "\n",
        "    network_learned = batch_loss < valid_loss_min\n",
        "\n",
        "    if network_learned:\n",
        "          valid_loss_min = batch_loss\n",
        "          torch.save(model.state_dict(), 'resnet.pt')\n",
        "          print('Improvement-Detected, save-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "438109a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Evaluate Model\n",
        "batch_sze = 10 #SET TO MAX OF WHAT UR COMPUTER CAN HANDLE\n",
        "model_path = \"/Users/em.ily/Documents/GitHub/FIT3170_Usability_Accessibility_Testing_App/pipeline/trained_models/resnet_v3.pt\"\n",
        "model_eval = ResNet(18, Block, 4, 2)\n",
        "model_eval.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))) #change to gpu if you have\n",
        "model_eval.eval()\n",
        "\n",
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Dataset\n",
        "dataset_eval = dataset[dataset['split']=='eval']\n",
        "eval_dataset = Tappable(dataset= dataset_eval,\n",
        "                        root_dir= img_dir,\n",
        "                        transform=transforms.Compose([\n",
        "                        applyMask(),\n",
        "                        ToTensor()\n",
        "                        ]))\n",
        "\n",
        "dataloader_eval = DataLoader(eval_dataset,batch_size=batch_sze, shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)\n",
        "batch_loss = []\n",
        "output_target = []\n",
        "output_pred = [] \n",
        "for batch_idx, item in enumerate(dataloader_eval):\n",
        "      data_t, target_t = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "      for item in target_t.tolist():\n",
        "            output_target.append(item)\n",
        "      outputs_t = model_eval(data_t)\n",
        "      for item in outputs_t:\n",
        "            _, index = torch.max(outputs_t, 1) \n",
        "            output_pred.append(index[0])\n",
        "      loss_t = criterion(outputs_t, target_t)\n",
        "      batch_loss.append(loss_t.item())\n",
        "      # print('Test Loss: {:.4f}'.format(loss_t.item()))\n",
        "# print('Overall Test Loss: {:.4f}'.format(np.mean(batch_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ea6f7f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6139)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1 = F1Score(num_classes=6)\n",
        "pred_tensor = torch.Tensor(output_pred).int()\n",
        "target_tensor =  torch.Tensor(output_target).int()\n",
        "f1(pred_tensor, target_tensor)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tappable.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 ('fit3170')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "db9f884420d89a79bf1728617c543a9611c152edc12e7810ea01e99d40f0dfd0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
