{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lRIs1xKv9Cqb",
      "metadata": {
        "id": "lRIs1xKv9Cqb"
      },
      "source": [
        "https://github.com/google-research/google-research/tree/master/taperception\n",
        "\n",
        "https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517497#BibPLXBIB0037\n",
        "\n",
        "Adding notebook to github for version control. \n",
        "Download json_dir and image_dir from google drive to local, or run in Google colab and mount drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "hTr0SCVXoV7L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTr0SCVXoV7L",
        "outputId": "4e57ba9c-df1e-4e84-f9c4-36a5fc0ff50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (0.13.1)\n",
            "Requirement already satisfied: torchmetrics in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: numpy in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (1.23.1)\n",
            "Requirement already satisfied: pandas in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (1.4.3)\n",
            "Requirement already satisfied: scikit-image in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (0.19.3)\n",
            "Requirement already satisfied: torch in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from torchvision) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
            "Requirement already satisfied: requests in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
            "Requirement already satisfied: packaging in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from scikit-image) (2022.8.8)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from scikit-image) (1.9.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from scikit-image) (2.21.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from scikit-image) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from scikit-image) (2.8.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/em.ily/miniforge3/envs/fit3170/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision torchmetrics numpy pandas scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ec2d4847",
      "metadata": {
        "id": "ec2d4847"
      },
      "outputs": [],
      "source": [
        "#Imports \n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "import torchvision.models as models\n",
        "from torchmetrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "BA1l0kE4xf7L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA1l0kE4xf7L",
        "outputId": "0294552a-f853-455e-f661-4398691228ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x12cd80970>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcf032a1",
      "metadata": {
        "id": "fcf032a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14781"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/google-research-datasets/taperception/main/rico_tap_annotations_idsonly.csv')\n",
        "dataset_test = dataset[dataset['split']=='test']\n",
        "dataset_train = dataset[dataset['split']=='train']\n",
        "\n",
        "json_dir = \"/Users/em.ily/Downloads/json/\"\n",
        "img_dir = \"/Users/em.ily/Downloads/combined/\"\n",
        "\n",
        "len(dataset_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ef5bf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#HYPERPARAMETER TURNING & DATASET SIZE\n",
        "\n",
        "# #full dataset\n",
        "# batch_sze = 1024\n",
        "# epoch_sze = 1500\n",
        "\n",
        "#smaller dataset (change values as you please)\n",
        "batch_sze = 1\n",
        "epoch_sze = 15\n",
        "dataset_test = dataset_test[0:int(np.ceil(len(dataset_test)/10))].reset_index(drop=True)\n",
        "dataset_train = dataset_train[0:int(np.ceil(len(dataset_train))/10)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdb0c42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ebdb0c42",
        "outputId": "fcd7a926-eb22-4de8-d068-be2087df78ef"
      },
      "outputs": [],
      "source": [
        "dataset_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a611fac",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39c2c58e",
      "metadata": {
        "id": "39c2c58e"
      },
      "outputs": [],
      "source": [
        "class image:\n",
        "    \"\"\"\n",
        "    stores the image bounds and related json file to each image. \n",
        "    stores all existing image objects in array. \n",
        "    \"\"\"\n",
        "\n",
        "    _all = []\n",
        "    \n",
        "    def __init__(self, image_id, json_file):\n",
        "        self.image_id = image_id\n",
        "        self.json_file = json_file\n",
        "        self.image_bounds = self.json_file['activity']['root']['rel-bounds']\n",
        "        self.image_bounds_height = self.image_bounds[3]\n",
        "        self.image_bounds_width = self.image_bounds[2]\n",
        "\n",
        "        image._all.append(self)\n",
        "\n",
        "    @classmethod\n",
        "    def all(cls):\n",
        "        return cls._all\n",
        "    \n",
        "    def get_image_bounds_height(self):\n",
        "        return self.image_bounds_height\n",
        "\n",
        "    def get_image_bounds_width(self):\n",
        "        return self.image_bounds_width\n",
        "\n",
        "    def get_json_file(self):\n",
        "        return self.json_file\n",
        "\n",
        "    def list_objects(self):\n",
        "        \"\"\"\n",
        "        returns list of all objects owned by image\n",
        "        \"\"\"\n",
        "        image_objects = [img_obj for img_obj in img_obj.all() if img_obj.owner == self]\n",
        "        return image_objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f169236c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class img_obj:\n",
        "    \"\"\"\n",
        "    finds the bounds of an object within an image using the image json file. \n",
        "    stores object bounds. \n",
        "    stores all existing objects in array.\n",
        "    \"\"\"\n",
        "\n",
        "    _all = []\n",
        "\n",
        "    def __init__(self, object_id, image):\n",
        "        #object\n",
        "        self.object_bounds = []\n",
        "        self.object_id = object_id\n",
        "        self.image = image\n",
        "\n",
        "        img_obj._all.append(self)\n",
        "        self.find_object_bounds(self.image.get_json_file()) #calls method to find object bounds\n",
        "\n",
        "    @classmethod\n",
        "    def all(cls):\n",
        "        return cls._all\n",
        "\n",
        "    def find_object_bounds(self, dict_file):\n",
        "        \"\"\"\n",
        "        recurring method which loops through dictionary file to find the object id and returns bounds\n",
        "        TODO: fix this method. there has to be a better way. \n",
        "        \"\"\"\n",
        "        for key in dict_file:\n",
        "            if key == 'pointer' and dict_file['pointer'] == self.object_id:\n",
        "                self.set_object_bounds(dict_file['bounds'])\n",
        "            if isinstance(dict_file[key], dict):\n",
        "                self.find_object_bounds(dict_file[key])\n",
        "            elif isinstance(dict_file[key], list):\n",
        "                self.find_object_bounds_lst(dict_file[key])\n",
        "\n",
        "    def find_object_bounds_lst(self, lst):\n",
        "        \"\"\"\n",
        "        recurring method which loops through list to find the object id and return bounds\n",
        "        \"\"\"\n",
        "        for item in lst:\n",
        "            if isinstance(item, dict):\n",
        "                self.find_object_bounds(item)\n",
        "            elif isinstance(item, list):\n",
        "                self.find_object_bounds_lst(item)\n",
        "\n",
        "    def get_object_bounds(self):\n",
        "        return self.object_bounds\n",
        "\n",
        "    def set_object_bounds(self, new_bounds):\n",
        "        self.object_bounds = new_bounds\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "691e0e84",
      "metadata": {
        "id": "691e0e84"
      },
      "outputs": [],
      "source": [
        "class applyMask(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Matrix multiplication of the RGB image and a binary mask of the object\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \n",
        "        image = sample['image']\n",
        "        \n",
        "        object_bounds, height, width = self.get_bounds(sample['image_id'], sample['object_id'])\n",
        "        \n",
        "        binary_mask = np.zeros(shape=(image.shape[0], image.shape[1]))\n",
        "        x_ratio_min = object_bounds[0]/width\n",
        "        x_ratio_max = object_bounds[2]/width\n",
        "        y_ratio_min = object_bounds[1]/height\n",
        "        y_ratio_max = object_bounds[3]/height\n",
        "        \n",
        "        for x in range(image.shape[0]):\n",
        "            for y in range(image.shape[1]):\n",
        "                if x_ratio_min <= x/image.shape[0] < x_ratio_max and y_ratio_min <= y/image.shape[1] < y_ratio_max:\n",
        "                    binary_mask[x,y] = 1 #sets binary mask value to 1 if within tappable bounds\n",
        "        concat = np.dstack((image, binary_mask)) #matrix multiplication of image and binary mask\n",
        "        return {'image': concat, 'label': sample['label']}\n",
        "\n",
        "    def get_bounds(self, image_id, object_id):\n",
        "        \"\"\"\n",
        "        Returns bounds of the object and image\n",
        "        \"\"\"\n",
        "\n",
        "        #loads json file for specific image id\n",
        "        json_file = open(json_dir + str(image_id) + '.json') \n",
        "        image_json = json.load(json_file)\n",
        "\n",
        "        #checks if image obect is created \n",
        "        image_match = [image for image in image.all() if image.image_id == image_id]\n",
        "        if len(image_match) == 0:\n",
        "            #if no, create image object\n",
        "            img = image(image_id, image_json)\n",
        "        else:\n",
        "            img = image_match[0]\n",
        "        obj = img_obj(object_id, img) #create object\n",
        "        return obj.get_object_bounds(), img.get_image_bounds_height(), img.get_image_bounds_width()\n",
        "        \n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'label': label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1a1e7f4a",
      "metadata": {
        "id": "1a1e7f4a"
      },
      "outputs": [],
      "source": [
        "class Tappable(Dataset):\n",
        "    \"\"\"\n",
        "    Creates dataset from the csv of labelled image and object ids\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,root_dir, dataset, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image_id = self.dataset.iloc[idx, 0]\n",
        "        object_id = self.dataset.iloc[idx, 1]\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                str(image_id) + \".jpg\", )\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        image = transform.resize(image, (540, 960))\n",
        "        \n",
        "        label = self.dataset.iloc[idx, 2]\n",
        "        \n",
        "        sample = {'image': image, 'image_id': image_id, 'object_id': object_id, 'label': int(label) }\n",
        "        \n",
        "        if self.transform:\n",
        "            sample_out = self.transform(sample)\n",
        "\n",
        "        return sample_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "gbq-rDlFYQ69",
      "metadata": {
        "id": "gbq-rDlFYQ69"
      },
      "outputs": [],
      "source": [
        "#resnet18 cnn model \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.expansion = 1\n",
        "        # self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        # self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.expansion = 1\n",
        "        layers = [2, 2, 2, 2] #resnet 18\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNetLayers\n",
        "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        layers = []\n",
        "\n",
        "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
        "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
        "        self.in_channels = intermediate_channels * self.expansion\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464161d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Tappable(dataset= dataset_train,\n",
        "                         root_dir= img_dir,\n",
        "                         transform=transforms.Compose([\n",
        "                         applyMask(),\n",
        "                         ToTensor()\n",
        "                         ]))\n",
        "\n",
        "test_dataset = Tappable(dataset= dataset_test,\n",
        "                        root_dir= img_dir,\n",
        "                        transform=transforms.Compose([\n",
        "                        applyMask(),\n",
        "                        ToTensor()\n",
        "                        ]))\n",
        "#batch size should be 1024\n",
        "dataloader_train = DataLoader(train_dataset,batch_size=batch_sze, shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)\n",
        "dataloader_test = DataLoader(test_dataset,batch_size=batch_sze,shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616441da",
      "metadata": {
        "id": "616441da"
      },
      "outputs": [],
      "source": [
        "model = ResNet(18, Block, 4, 2)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#lr - reduces epoch by factor 10 at specific epoch times\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum = 0.9)\n",
        "lambda_lr = lambda epoch: 0.1 ** epoch if epoch%==0 else 1\n",
        "lr_scheduler = optim.lr_scheduler.LambdaLR(\n",
        "   optimizer=optimizer,\n",
        "   lr_lambda = lambda_lr\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89da90f1",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "89da90f1",
        "outputId": "a4293eeb-724d-46b3-f1ef-fad32583ece1"
      },
      "outputs": [],
      "source": [
        "n_epochs = epoch_sze\n",
        "test_loss = []\n",
        "train_loss = []\n",
        "total_step_train = len(dataloader_train)\n",
        "total_step_test = len(dataloader_test)\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f'Epoch {epoch}\\n')\n",
        "    for batch_idx, item in enumerate(dataloader_train):\n",
        "        inputs, labels = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx) % 20 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}'\n",
        "                   .format(epoch, n_epochs, batch_idx, total_step_train, loss.item()))\n",
        "    train_loss.append(running_loss/total_step_train)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    batch_loss = 0\n",
        "    for batch_idx, item in enumerate(dataloader_test):\n",
        "      data_t, target_t = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "      outputs_t = model(data_t)\n",
        "      loss_t = criterion(outputs_t, target_t)\n",
        "      batch_loss += loss_t.item()\n",
        "      if (batch_idx) % 20 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Test Loss: {:.4f}'\n",
        "                   .format(epoch, n_epochs, batch_idx, total_step_test, loss_t.item()))\n",
        "\n",
        "    test_loss.append(batch_loss/total_step_test)\n",
        "\n",
        "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, test-loss: {np.mean(test_loss):.4f}')\n",
        "\n",
        "    network_learned = batch_loss < valid_loss_min\n",
        "\n",
        "    if network_learned:\n",
        "          valid_loss_min = batch_loss\n",
        "          torch.save(model.state_dict(), 'resnet.pt')\n",
        "          print('Improvement-Detected, save-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "438109a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.4551\n",
            "Test Loss: 2.2245\n",
            "Test Loss: 1.7877\n",
            "Test Loss: 3.5011\n",
            "Test Loss: 0.8746\n",
            "Test Loss: 0.9516\n",
            "Test Loss: 0.0717\n",
            "Test Loss: 2.8442\n",
            "Test Loss: 1.7909\n",
            "Test Loss: 2.1162\n",
            "Test Loss: 0.9924\n",
            "Test Loss: 1.7220\n",
            "Test Loss: 1.3826\n",
            "Test Loss: 1.2889\n",
            "Test Loss: 2.0433\n",
            "Test Loss: 2.6609\n",
            "Test Loss: 1.4625\n",
            "Test Loss: 2.3361\n",
            "Test Loss: 1.4392\n",
            "Test Loss: 1.4335\n",
            "Test Loss: 2.2989\n",
            "Test Loss: 0.3203\n",
            "Test Loss: 1.1938\n",
            "Test Loss: 3.0997\n",
            "Test Loss: 1.4668\n",
            "Test Loss: 1.4827\n",
            "Test Loss: 1.8421\n",
            "Test Loss: 0.8260\n",
            "Test Loss: 2.3743\n",
            "Test Loss: 4.5241\n",
            "Test Loss: 2.6560\n",
            "Test Loss: 1.1496\n",
            "Test Loss: 1.1585\n",
            "Test Loss: 2.9785\n",
            "Test Loss: 3.2070\n",
            "Test Loss: 1.0985\n",
            "Test Loss: 1.1594\n",
            "Test Loss: 0.8521\n",
            "Test Loss: 1.4467\n",
            "Test Loss: 1.7727\n",
            "Test Loss: 0.7640\n",
            "Test Loss: 1.0449\n",
            "Test Loss: 1.4284\n",
            "Test Loss: 1.6161\n",
            "Test Loss: 0.4701\n",
            "Test Loss: 2.0167\n",
            "Test Loss: 1.6832\n",
            "Test Loss: 0.3394\n",
            "Test Loss: 1.7741\n",
            "Test Loss: 0.7893\n",
            "Test Loss: 1.6949\n",
            "Test Loss: 1.9855\n",
            "Test Loss: 2.0252\n",
            "Test Loss: 0.5746\n",
            "Test Loss: 1.6944\n",
            "Test Loss: 0.7786\n",
            "Test Loss: 0.0157\n",
            "Test Loss: 3.0829\n",
            "Test Loss: 0.8348\n",
            "Test Loss: 1.3375\n",
            "Test Loss: 1.7641\n",
            "Test Loss: 2.1297\n",
            "Test Loss: 1.2550\n",
            "Test Loss: 0.9294\n",
            "Test Loss: 3.1062\n",
            "Test Loss: 0.8707\n",
            "Test Loss: 1.8077\n",
            "Test Loss: 0.9498\n",
            "Test Loss: 1.9720\n",
            "Test Loss: 3.0529\n",
            "Test Loss: 2.7868\n",
            "Test Loss: 0.1028\n",
            "Test Loss: 0.8622\n",
            "Test Loss: 1.1227\n",
            "Test Loss: 1.4525\n",
            "Test Loss: 0.3538\n",
            "Test Loss: 1.5386\n",
            "Test Loss: 0.8649\n",
            "Test Loss: 1.8492\n",
            "Test Loss: 1.5662\n",
            "Test Loss: 2.1839\n",
            "Test Loss: 1.0490\n",
            "Test Loss: 0.9005\n",
            "Test Loss: 1.3114\n",
            "Test Loss: 1.9566\n",
            "Test Loss: 2.4320\n",
            "Test Loss: 1.2754\n",
            "Test Loss: 2.7604\n",
            "Test Loss: 0.8800\n",
            "Test Loss: 2.0644\n",
            "Test Loss: 1.3048\n",
            "Test Loss: 0.7970\n",
            "Test Loss: 2.9793\n",
            "Test Loss: 1.8183\n",
            "Test Loss: 0.6223\n",
            "Test Loss: 1.3330\n",
            "Test Loss: 1.9157\n",
            "Test Loss: 1.9437\n",
            "Test Loss: 0.8539\n",
            "Test Loss: 2.7019\n",
            "Test Loss: 0.8726\n",
            "Test Loss: 0.4874\n",
            "Test Loss: 0.8442\n",
            "Test Loss: 2.1536\n",
            "Test Loss: 1.6872\n",
            "Test Loss: 0.9153\n",
            "Test Loss: 0.5774\n",
            "Test Loss: 1.7440\n",
            "Test Loss: 0.4235\n",
            "Test Loss: 1.6803\n",
            "Test Loss: 1.8031\n",
            "Test Loss: 1.7956\n",
            "Test Loss: 2.2050\n",
            "Test Loss: 2.4079\n",
            "Test Loss: 2.0669\n",
            "Test Loss: 2.0810\n",
            "Test Loss: 3.4804\n",
            "Test Loss: 1.6524\n",
            "Test Loss: 1.1124\n",
            "Test Loss: 0.4956\n",
            "Test Loss: 0.6336\n",
            "Test Loss: 1.6026\n",
            "Test Loss: 1.7614\n",
            "Test Loss: 1.1377\n",
            "Test Loss: 2.3145\n",
            "Test Loss: 0.8975\n",
            "Test Loss: 2.0133\n",
            "Test Loss: 0.8549\n",
            "Test Loss: 1.5060\n",
            "Test Loss: 1.7178\n",
            "Test Loss: 1.6724\n",
            "Test Loss: 1.4577\n",
            "Test Loss: 1.5324\n",
            "Test Loss: 2.0968\n",
            "Test Loss: 1.6133\n",
            "Test Loss: 1.3627\n",
            "Test Loss: 1.0854\n",
            "Test Loss: 1.4998\n",
            "Test Loss: 3.0624\n",
            "Test Loss: 1.8361\n",
            "Test Loss: 2.3204\n",
            "Test Loss: 2.4242\n",
            "Test Loss: 0.9864\n",
            "Test Loss: 0.8255\n",
            "Test Loss: 1.3427\n",
            "Test Loss: 1.7623\n",
            "Test Loss: 2.1749\n",
            "Test Loss: 0.5503\n",
            "Test Loss: 1.3673\n",
            "Test Loss: 1.4370\n",
            "Test Loss: 0.6680\n",
            "Test Loss: 2.5369\n",
            "Test Loss: 4.9159\n",
            "Test Loss: 1.6798\n",
            "Test Loss: 1.6419\n",
            "Test Loss: 1.8465\n",
            "Test Loss: 1.5436\n",
            "Test Loss: 1.4797\n",
            "Test Loss: 2.8865\n",
            "Test Loss: 2.9397\n",
            "Test Loss: 1.2467\n",
            "Test Loss: 1.1560\n",
            "Test Loss: 1.0686\n",
            "Test Loss: 3.2875\n",
            "Test Loss: 0.9544\n",
            "Test Loss: 2.7847\n",
            "Test Loss: 0.9498\n",
            "Test Loss: 1.4755\n",
            "Test Loss: 0.4420\n",
            "Test Loss: 0.6752\n",
            "Test Loss: 1.2959\n",
            "Test Loss: 0.6585\n",
            "Test Loss: 2.2084\n",
            "Test Loss: 2.4497\n",
            "Test Loss: 1.0782\n",
            "Test Loss: 1.4960\n",
            "Test Loss: 1.6795\n",
            "Test Loss: 3.3878\n",
            "Test Loss: 2.3951\n",
            "Test Loss: 0.9860\n",
            "Test Loss: 0.5676\n",
            "Test Loss: 2.0510\n",
            "Test Loss: 1.0756\n",
            "Test Loss: 0.8906\n",
            "Test Loss: 1.4455\n",
            "Test Loss: 0.8653\n",
            "Overall Test Loss: 1.6113\n"
          ]
        }
      ],
      "source": [
        "#Evaluate Model\n",
        "batch_sze = 10 #SET TO MAX OF WHAT UR COMPUTER CAN HANDLE\n",
        "model_path = \"/trained_models/resnet_v2.pt\"\n",
        "model_eval = ResNet(18, Block, 4, 2)\n",
        "model_eval.load_state_dict(torch.load(os.getcwd() + model_path, map_location=torch.device('cpu'))) #change to gpu if you have\n",
        "model_eval.eval()\n",
        "\n",
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Dataset\n",
        "dataset_eval = dataset[dataset['split']=='eval']\n",
        "eval_dataset = Tappable(dataset= dataset_eval,\n",
        "                        root_dir= img_dir,\n",
        "                        transform=transforms.Compose([\n",
        "                        applyMask(),\n",
        "                        ToTensor()\n",
        "                        ]))\n",
        "\n",
        "dataloader_eval = DataLoader(eval_dataset,batch_size=batch_sze, shuffle=True, pin_memory=True if DEVICE == \"cuda\" else False)\n",
        "batch_loss = []\n",
        "output_target = []\n",
        "output_pred = [] \n",
        "for batch_idx, item in enumerate(dataloader_eval):\n",
        "      data_t, target_t = item['image'].type(torch.FloatTensor).to(DEVICE), item['label'].type(torch.LongTensor).to(DEVICE)\n",
        "      for item in target_t.tolist():\n",
        "            output_target.append(item)\n",
        "      outputs_t = model_eval(data_t)\n",
        "      for item in outputs_t:\n",
        "            _, index = torch.max(outputs_t, 1) \n",
        "            output_pred.append(index[0])\n",
        "      loss_t = criterion(outputs_t, target_t)\n",
        "      batch_loss.append(loss_t.item())\n",
        "      print('Test Loss: {:.4f}'.format(loss_t.item()))\n",
        "print('Overall Test Loss: {:.4f}'.format(np.mean(batch_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "75ea6f7f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6107)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1 = F1Score(num_classes=6)\n",
        "pred_tensor = torch.Tensor(output_pred).int()\n",
        "target_tensor =  torch.Tensor(output_target).int()\n",
        "f1(pred_tensor, target_tensor)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tappable.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 ('fit3170')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "db9f884420d89a79bf1728617c543a9611c152edc12e7810ea01e99d40f0dfd0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
